{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Flower_Classification_Pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiNQJ8MB4lpb"
      },
      "source": [
        "'''\r\n",
        "Flower classification using PyTorch\r\n",
        "dataset: https://www.kaggle.com/alxmamaev/flowers-recognition\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wum7h_REttic",
        "outputId": "95c335eb-80d8-4a40-92b2-4c418e85fd93"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4udW9sWvwSho"
      },
      "source": [
        "import os\r\n",
        "\r\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/Kaggle\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "KZBOt6Mdw8D0",
        "outputId": "93ff11ae-7764-452f-8cd9-a6ff5bdf8c48"
      },
      "source": [
        "#changing the working directory\r\n",
        "%cd /content/gdrive/My Drive/Kaggle\r\n",
        "%pwd"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Kaggle\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/gdrive/My Drive/Kaggle'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3BHGYToxCOp",
        "outputId": "757ff6a8-2cef-4dc4-ea92-b44aadefc3cb"
      },
      "source": [
        "!kaggle datasets download -d alxmamaev/flowers-recognition"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading flowers-recognition.zip to /content/gdrive/My Drive/Kaggle\n",
            " 97% 436M/450M [00:03<00:00, 151MB/s]\n",
            "100% 450M/450M [00:03<00:00, 139MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZQrh7K4xMRE"
      },
      "source": [
        "#unzipping the zip files and deleting the zip files\r\n",
        "!unzip \\*.zip  && rm *.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTo2cGzmxc0v"
      },
      "source": [
        "DIR_PATH = '/content/gdrive/MyDrive/Kaggle/flowers/flowers'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyUmenUjyc5V"
      },
      "source": [
        "import os\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "# pytorch imports\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torchvision\r\n",
        "from torch.utils.data import Dataset,DataLoader\r\n",
        "import torchvision.transforms as transforms\r\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T21fxJY7ywXm"
      },
      "source": [
        "mean = np.array([0.485, 0.456, 0.406])\r\n",
        "std = np.array([0.229, 0.224, 0.225])\r\n",
        "\r\n",
        "transformations = {\r\n",
        "    'train': transforms.Compose([\r\n",
        "        transforms.Resize((224,224)),\r\n",
        "        transforms.CenterCrop((224,224)),\r\n",
        "        transforms.RandomHorizontalFlip(),\r\n",
        "        transforms.ColorJitter(),\r\n",
        "        transforms.ToTensor(),\r\n",
        "        transforms.Normalize(mean,std)\r\n",
        "    ]),\r\n",
        "    'test': transforms.Compose([\r\n",
        "        transforms.Resize((224,224)),\r\n",
        "        transforms.CenterCrop((224,224)),\r\n",
        "        transforms.ToTensor(),\r\n",
        "        transforms.Normalize(mean,std)\r\n",
        "    ])\r\n",
        "}"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4l-8TGRAy0RT",
        "outputId": "d3705cbd-2588-4307-ca10-1e60dadff313"
      },
      "source": [
        "# hyperparamters\r\n",
        "\r\n",
        "learning_rate = 0.001\r\n",
        "batch_size = 8\r\n",
        "num_epochs = 50\r\n",
        "num_classes = 5\r\n",
        "\r\n",
        "# device\r\n",
        "device = None\r\n",
        "if torch.cuda.is_available():\r\n",
        "    device = torch.device('cuda')\r\n",
        "else:\r\n",
        "    device = torch.device('cpu')\r\n",
        "    \r\n",
        "print(device)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_ndKLo6y3yx",
        "outputId": "b78465ca-35ed-44fc-bb68-b28de8009d3a"
      },
      "source": [
        "total_dataset = torchvision.datasets.ImageFolder(DIR_PATH,transform=transformations['train'])\r\n",
        "\r\n",
        "len(total_dataset),total_dataset[0][0].shape,total_dataset.class_to_idx"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4323,\n",
              " torch.Size([3, 224, 224]),\n",
              " {'daisy': 0, 'dandelion': 1, 'rose': 2, 'sunflower': 3, 'tulip': 4})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdt7qTbGy_TT",
        "outputId": "e062ea74-ca5e-4560-a8b2-2580b76d44e5"
      },
      "source": [
        "# splitting into train and validation sets\r\n",
        "\r\n",
        "SPLIT_SIZE = 0.8\r\n",
        "tot_len = len(total_dataset)\r\n",
        "\r\n",
        "train_size = int(SPLIT_SIZE * tot_len)\r\n",
        "val_size = tot_len - train_size\r\n",
        "\r\n",
        "print(f'Training set size = {train_size} \\nValidation set size = {val_size}')\r\n",
        "\r\n",
        "train_dataset, val_dataset =  torch.utils.data.random_split(total_dataset,[train_size,val_size])\r\n",
        "\r\n",
        "len(train_dataset),len(val_dataset)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set size = 3458 \n",
            "Validation set size = 865\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3458, 865)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzn7elfVzDGe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59942c37-e670-45ef-e912-1e20efabf912"
      },
      "source": [
        "# dataloaders\r\n",
        "train_loader = DataLoader(dataset=train_dataset,\r\n",
        "                         batch_size=batch_size,\r\n",
        "                         shuffle=True,\r\n",
        "                         num_workers=4)\r\n",
        "\r\n",
        "val_loader = DataLoader(dataset=val_dataset,\r\n",
        "                       batch_size=1,\r\n",
        "                       shuffle=True,\r\n",
        "                       num_workers=4)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMCSKY0nzIak",
        "outputId": "0ff2e227-0dc2-48c1-870d-cd5236f75645"
      },
      "source": [
        "# testing dataloading \r\n",
        "\r\n",
        "examples = iter(train_loader)\r\n",
        "samples,labels = examples.next()\r\n",
        "print(samples.shape,labels.shape) # batch_size=8\r\n",
        "len(train_loader),len(val_loader)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([8, 3, 224, 224]) torch.Size([8])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(433, 865)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hk-Bdsw3zRdH"
      },
      "source": [
        "# custom CNN model class\r\n",
        "\r\n",
        "class ConvNet(nn.Module):\r\n",
        "    def __init__(self,model,num_classes):\r\n",
        "        super(ConvNet,self).__init__()\r\n",
        "        self.base_model = nn.Sequential(*list(model.children())[:-1]) # model excluding last FC layer\r\n",
        "        self.linear1 = nn.Linear(in_features=2048,out_features=512)\r\n",
        "        self.relu = nn.ReLU()\r\n",
        "        self.linear2 = nn.Linear(in_features=512,out_features=num_classes)\r\n",
        "    \r\n",
        "    def forward(self,x):\r\n",
        "        x = self.base_model(x)\r\n",
        "        x = torch.flatten(x,1)\r\n",
        "        lin = self.linear1(x)\r\n",
        "        x = self.relu(lin)\r\n",
        "        out = self.linear2(x)\r\n",
        "        return lin, out"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RMBgE8rzYgE"
      },
      "source": [
        "model = torchvision.models.resnet50(pretrained=True) # base model\r\n",
        "\r\n",
        "model = ConvNet(model,num_classes)\r\n",
        "\r\n",
        "model = model.to(device)\r\n",
        "\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate,momentum=0.9)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DcGQdCuzdKA",
        "outputId": "79c95e2b-85fe-485f-a0cd-8828f5fb715c"
      },
      "source": [
        "print(model)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ConvNet(\n",
            "  (base_model): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (4): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (5): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (3): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (6): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (3): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (4): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (5): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (7): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  )\n",
            "  (linear1): Linear(in_features=2048, out_features=512, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (linear2): Linear(in_features=512, out_features=5, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IRN4zwkzhov",
        "outputId": "4dba26b0-9b19-40fb-9714-e1975a530ba2"
      },
      "source": [
        "# training loop\r\n",
        "\r\n",
        "n_iters = len(train_loader)\r\n",
        "\r\n",
        "for epoch in range(num_epochs):\r\n",
        "    model.train()\r\n",
        "    for ii,(images,labels) in enumerate(train_loader):\r\n",
        "        images = images.to(device)\r\n",
        "        labels = labels.to(device)\r\n",
        "        \r\n",
        "        _,outputs = model(images)\r\n",
        "        loss = criterion(outputs,labels)\r\n",
        "        \r\n",
        "        # free_gpu_cache()\r\n",
        "        \r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "        optimizer.zero_grad()\r\n",
        "        \r\n",
        "        if (ii+1)%108 == 0:\r\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{ii+1}/{n_iters}], Loss = {loss.item():.6f}')\r\n",
        "            \r\n",
        "    print('----------------------------------------')\r\n",
        "    "
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/50], Step [108/433], Loss = 0.757020\n",
            "Epoch [1/50], Step [216/433], Loss = 0.685718\n",
            "Epoch [1/50], Step [324/433], Loss = 0.115149\n",
            "Epoch [1/50], Step [432/433], Loss = 0.439111\n",
            "----------------------------------------\n",
            "Epoch [2/50], Step [108/433], Loss = 0.136134\n",
            "Epoch [2/50], Step [216/433], Loss = 0.049118\n",
            "Epoch [2/50], Step [324/433], Loss = 0.456234\n",
            "Epoch [2/50], Step [432/433], Loss = 0.104673\n",
            "----------------------------------------\n",
            "Epoch [3/50], Step [108/433], Loss = 0.034409\n",
            "Epoch [3/50], Step [216/433], Loss = 0.231488\n",
            "Epoch [3/50], Step [324/433], Loss = 0.066696\n",
            "Epoch [3/50], Step [432/433], Loss = 0.467270\n",
            "----------------------------------------\n",
            "Epoch [4/50], Step [108/433], Loss = 0.026214\n",
            "Epoch [4/50], Step [216/433], Loss = 0.030503\n",
            "Epoch [4/50], Step [324/433], Loss = 0.312166\n",
            "Epoch [4/50], Step [432/433], Loss = 0.314792\n",
            "----------------------------------------\n",
            "Epoch [5/50], Step [108/433], Loss = 0.021850\n",
            "Epoch [5/50], Step [216/433], Loss = 1.182730\n",
            "Epoch [5/50], Step [324/433], Loss = 0.010956\n",
            "Epoch [5/50], Step [432/433], Loss = 0.320921\n",
            "----------------------------------------\n",
            "Epoch [6/50], Step [108/433], Loss = 0.024623\n",
            "Epoch [6/50], Step [216/433], Loss = 0.063280\n",
            "Epoch [6/50], Step [324/433], Loss = 0.058266\n",
            "Epoch [6/50], Step [432/433], Loss = 0.255990\n",
            "----------------------------------------\n",
            "Epoch [7/50], Step [108/433], Loss = 0.004076\n",
            "Epoch [7/50], Step [216/433], Loss = 0.001989\n",
            "Epoch [7/50], Step [324/433], Loss = 0.019225\n",
            "Epoch [7/50], Step [432/433], Loss = 0.002971\n",
            "----------------------------------------\n",
            "Epoch [8/50], Step [108/433], Loss = 0.010881\n",
            "Epoch [8/50], Step [216/433], Loss = 0.031959\n",
            "Epoch [8/50], Step [324/433], Loss = 0.001291\n",
            "Epoch [8/50], Step [432/433], Loss = 0.012870\n",
            "----------------------------------------\n",
            "Epoch [9/50], Step [108/433], Loss = 0.046522\n",
            "Epoch [9/50], Step [216/433], Loss = 0.069073\n",
            "Epoch [9/50], Step [324/433], Loss = 0.012408\n",
            "Epoch [9/50], Step [432/433], Loss = 0.258285\n",
            "----------------------------------------\n",
            "Epoch [10/50], Step [108/433], Loss = 0.143111\n",
            "Epoch [10/50], Step [216/433], Loss = 0.001320\n",
            "Epoch [10/50], Step [324/433], Loss = 0.003451\n",
            "Epoch [10/50], Step [432/433], Loss = 0.013060\n",
            "----------------------------------------\n",
            "Epoch [11/50], Step [108/433], Loss = 0.004306\n",
            "Epoch [11/50], Step [216/433], Loss = 0.013547\n",
            "Epoch [11/50], Step [324/433], Loss = 0.000519\n",
            "Epoch [11/50], Step [432/433], Loss = 0.004515\n",
            "----------------------------------------\n",
            "Epoch [12/50], Step [108/433], Loss = 0.000471\n",
            "Epoch [12/50], Step [216/433], Loss = 0.002339\n",
            "Epoch [12/50], Step [324/433], Loss = 0.002406\n",
            "Epoch [12/50], Step [432/433], Loss = 0.003472\n",
            "----------------------------------------\n",
            "Epoch [13/50], Step [108/433], Loss = 0.001540\n",
            "Epoch [13/50], Step [216/433], Loss = 0.006981\n",
            "Epoch [13/50], Step [324/433], Loss = 0.051327\n",
            "Epoch [13/50], Step [432/433], Loss = 0.001152\n",
            "----------------------------------------\n",
            "Epoch [14/50], Step [108/433], Loss = 0.015160\n",
            "Epoch [14/50], Step [216/433], Loss = 0.008562\n",
            "Epoch [14/50], Step [324/433], Loss = 0.007153\n",
            "Epoch [14/50], Step [432/433], Loss = 0.001574\n",
            "----------------------------------------\n",
            "Epoch [15/50], Step [108/433], Loss = 0.055420\n",
            "Epoch [15/50], Step [216/433], Loss = 0.003742\n",
            "Epoch [15/50], Step [324/433], Loss = 0.027467\n",
            "Epoch [15/50], Step [432/433], Loss = 0.000873\n",
            "----------------------------------------\n",
            "Epoch [16/50], Step [108/433], Loss = 0.020957\n",
            "Epoch [16/50], Step [216/433], Loss = 0.003599\n",
            "Epoch [16/50], Step [324/433], Loss = 0.008234\n",
            "Epoch [16/50], Step [432/433], Loss = 0.006931\n",
            "----------------------------------------\n",
            "Epoch [17/50], Step [108/433], Loss = 0.027260\n",
            "Epoch [17/50], Step [216/433], Loss = 0.003029\n",
            "Epoch [17/50], Step [324/433], Loss = 0.061476\n",
            "Epoch [17/50], Step [432/433], Loss = 0.007669\n",
            "----------------------------------------\n",
            "Epoch [18/50], Step [108/433], Loss = 0.045105\n",
            "Epoch [18/50], Step [216/433], Loss = 0.073035\n",
            "Epoch [18/50], Step [324/433], Loss = 0.000374\n",
            "Epoch [18/50], Step [432/433], Loss = 0.009276\n",
            "----------------------------------------\n",
            "Epoch [19/50], Step [108/433], Loss = 0.003742\n",
            "Epoch [19/50], Step [216/433], Loss = 0.003483\n",
            "Epoch [19/50], Step [324/433], Loss = 0.018717\n",
            "Epoch [19/50], Step [432/433], Loss = 0.010006\n",
            "----------------------------------------\n",
            "Epoch [20/50], Step [108/433], Loss = 0.001704\n",
            "Epoch [20/50], Step [216/433], Loss = 0.515203\n",
            "Epoch [20/50], Step [324/433], Loss = 0.003981\n",
            "Epoch [20/50], Step [432/433], Loss = 0.000648\n",
            "----------------------------------------\n",
            "Epoch [21/50], Step [108/433], Loss = 0.011558\n",
            "Epoch [21/50], Step [216/433], Loss = 0.004467\n",
            "Epoch [21/50], Step [324/433], Loss = 0.000391\n",
            "Epoch [21/50], Step [432/433], Loss = 0.000377\n",
            "----------------------------------------\n",
            "Epoch [22/50], Step [108/433], Loss = 0.000102\n",
            "Epoch [22/50], Step [216/433], Loss = 0.001215\n",
            "Epoch [22/50], Step [324/433], Loss = 0.000408\n",
            "Epoch [22/50], Step [432/433], Loss = 0.002828\n",
            "----------------------------------------\n",
            "Epoch [23/50], Step [108/433], Loss = 0.000254\n",
            "Epoch [23/50], Step [216/433], Loss = 0.000289\n",
            "Epoch [23/50], Step [324/433], Loss = 0.001253\n",
            "Epoch [23/50], Step [432/433], Loss = 0.001899\n",
            "----------------------------------------\n",
            "Epoch [24/50], Step [108/433], Loss = 0.000090\n",
            "Epoch [24/50], Step [216/433], Loss = 0.002937\n",
            "Epoch [24/50], Step [324/433], Loss = 0.000239\n",
            "Epoch [24/50], Step [432/433], Loss = 0.000116\n",
            "----------------------------------------\n",
            "Epoch [25/50], Step [108/433], Loss = 0.009898\n",
            "Epoch [25/50], Step [216/433], Loss = 0.002452\n",
            "Epoch [25/50], Step [324/433], Loss = 0.001259\n",
            "Epoch [25/50], Step [432/433], Loss = 0.000210\n",
            "----------------------------------------\n",
            "Epoch [26/50], Step [108/433], Loss = 0.000264\n",
            "Epoch [26/50], Step [216/433], Loss = 0.000122\n",
            "Epoch [26/50], Step [324/433], Loss = 0.002029\n",
            "Epoch [26/50], Step [432/433], Loss = 0.001717\n",
            "----------------------------------------\n",
            "Epoch [27/50], Step [108/433], Loss = 0.006400\n",
            "Epoch [27/50], Step [216/433], Loss = 0.004616\n",
            "Epoch [27/50], Step [324/433], Loss = 0.344165\n",
            "Epoch [27/50], Step [432/433], Loss = 0.000307\n",
            "----------------------------------------\n",
            "Epoch [28/50], Step [108/433], Loss = 0.012129\n",
            "Epoch [28/50], Step [216/433], Loss = 0.024815\n",
            "Epoch [28/50], Step [324/433], Loss = 0.015291\n",
            "Epoch [28/50], Step [432/433], Loss = 0.001586\n",
            "----------------------------------------\n",
            "Epoch [29/50], Step [108/433], Loss = 0.007912\n",
            "Epoch [29/50], Step [216/433], Loss = 0.065062\n",
            "Epoch [29/50], Step [324/433], Loss = 0.000672\n",
            "Epoch [29/50], Step [432/433], Loss = 0.002701\n",
            "----------------------------------------\n",
            "Epoch [30/50], Step [108/433], Loss = 0.003527\n",
            "Epoch [30/50], Step [216/433], Loss = 0.004332\n",
            "Epoch [30/50], Step [324/433], Loss = 0.002320\n",
            "Epoch [30/50], Step [432/433], Loss = 0.004246\n",
            "----------------------------------------\n",
            "Epoch [31/50], Step [108/433], Loss = 0.015666\n",
            "Epoch [31/50], Step [216/433], Loss = 0.000067\n",
            "Epoch [31/50], Step [324/433], Loss = 0.000421\n",
            "Epoch [31/50], Step [432/433], Loss = 0.002420\n",
            "----------------------------------------\n",
            "Epoch [32/50], Step [108/433], Loss = 0.003504\n",
            "Epoch [32/50], Step [216/433], Loss = 0.000841\n",
            "Epoch [32/50], Step [324/433], Loss = 0.056101\n",
            "Epoch [32/50], Step [432/433], Loss = 0.000182\n",
            "----------------------------------------\n",
            "Epoch [33/50], Step [108/433], Loss = 0.001787\n",
            "Epoch [33/50], Step [216/433], Loss = 0.002021\n",
            "Epoch [33/50], Step [324/433], Loss = 0.001553\n",
            "Epoch [33/50], Step [432/433], Loss = 0.000463\n",
            "----------------------------------------\n",
            "Epoch [34/50], Step [108/433], Loss = 0.000158\n",
            "Epoch [34/50], Step [216/433], Loss = 0.000119\n",
            "Epoch [34/50], Step [324/433], Loss = 0.002823\n",
            "Epoch [34/50], Step [432/433], Loss = 0.000185\n",
            "----------------------------------------\n",
            "Epoch [35/50], Step [108/433], Loss = 0.000339\n",
            "Epoch [35/50], Step [216/433], Loss = 0.000566\n",
            "Epoch [35/50], Step [324/433], Loss = 0.001322\n",
            "Epoch [35/50], Step [432/433], Loss = 0.002154\n",
            "----------------------------------------\n",
            "Epoch [36/50], Step [108/433], Loss = 0.000214\n",
            "Epoch [36/50], Step [216/433], Loss = 0.000500\n",
            "Epoch [36/50], Step [324/433], Loss = 0.002063\n",
            "Epoch [36/50], Step [432/433], Loss = 0.008140\n",
            "----------------------------------------\n",
            "Epoch [37/50], Step [108/433], Loss = 0.000291\n",
            "Epoch [37/50], Step [216/433], Loss = 0.000081\n",
            "Epoch [37/50], Step [324/433], Loss = 0.000056\n",
            "Epoch [37/50], Step [432/433], Loss = 0.000171\n",
            "----------------------------------------\n",
            "Epoch [38/50], Step [108/433], Loss = 0.000020\n",
            "Epoch [38/50], Step [216/433], Loss = 0.000026\n",
            "Epoch [38/50], Step [324/433], Loss = 0.003402\n",
            "Epoch [38/50], Step [432/433], Loss = 0.000023\n",
            "----------------------------------------\n",
            "Epoch [39/50], Step [108/433], Loss = 0.000067\n",
            "Epoch [39/50], Step [216/433], Loss = 0.000043\n",
            "Epoch [39/50], Step [324/433], Loss = 0.001181\n",
            "Epoch [39/50], Step [432/433], Loss = 0.000114\n",
            "----------------------------------------\n",
            "Epoch [40/50], Step [108/433], Loss = 0.001593\n",
            "Epoch [40/50], Step [216/433], Loss = 0.000074\n",
            "Epoch [40/50], Step [324/433], Loss = 0.000158\n",
            "Epoch [40/50], Step [432/433], Loss = 0.002812\n",
            "----------------------------------------\n",
            "Epoch [41/50], Step [108/433], Loss = 0.000230\n",
            "Epoch [41/50], Step [216/433], Loss = 0.000792\n",
            "Epoch [41/50], Step [324/433], Loss = 0.029699\n",
            "Epoch [41/50], Step [432/433], Loss = 0.000284\n",
            "----------------------------------------\n",
            "Epoch [42/50], Step [108/433], Loss = 0.000945\n",
            "Epoch [42/50], Step [216/433], Loss = 0.000046\n",
            "Epoch [42/50], Step [324/433], Loss = 0.000987\n",
            "Epoch [42/50], Step [432/433], Loss = 0.002730\n",
            "----------------------------------------\n",
            "Epoch [43/50], Step [108/433], Loss = 0.001528\n",
            "Epoch [43/50], Step [216/433], Loss = 0.000332\n",
            "Epoch [43/50], Step [324/433], Loss = 0.000350\n",
            "Epoch [43/50], Step [432/433], Loss = 0.000211\n",
            "----------------------------------------\n",
            "Epoch [44/50], Step [108/433], Loss = 0.000792\n",
            "Epoch [44/50], Step [216/433], Loss = 0.000257\n",
            "Epoch [44/50], Step [324/433], Loss = 0.000683\n",
            "Epoch [44/50], Step [432/433], Loss = 0.008806\n",
            "----------------------------------------\n",
            "Epoch [45/50], Step [108/433], Loss = 0.001175\n",
            "Epoch [45/50], Step [216/433], Loss = 0.000702\n",
            "Epoch [45/50], Step [324/433], Loss = 0.000389\n",
            "Epoch [45/50], Step [432/433], Loss = 0.000090\n",
            "----------------------------------------\n",
            "Epoch [46/50], Step [108/433], Loss = 0.000136\n",
            "Epoch [46/50], Step [216/433], Loss = 0.000483\n",
            "Epoch [46/50], Step [324/433], Loss = 0.000011\n",
            "Epoch [46/50], Step [432/433], Loss = 0.000213\n",
            "----------------------------------------\n",
            "Epoch [47/50], Step [108/433], Loss = 0.000018\n",
            "Epoch [47/50], Step [216/433], Loss = 0.001065\n",
            "Epoch [47/50], Step [324/433], Loss = 0.000013\n",
            "Epoch [47/50], Step [432/433], Loss = 0.000744\n",
            "----------------------------------------\n",
            "Epoch [48/50], Step [108/433], Loss = 0.000017\n",
            "Epoch [48/50], Step [216/433], Loss = 0.000225\n",
            "Epoch [48/50], Step [324/433], Loss = 0.000096\n",
            "Epoch [48/50], Step [432/433], Loss = 0.000121\n",
            "----------------------------------------\n",
            "Epoch [49/50], Step [108/433], Loss = 0.066001\n",
            "Epoch [49/50], Step [216/433], Loss = 0.031501\n",
            "Epoch [49/50], Step [324/433], Loss = 0.000032\n",
            "Epoch [49/50], Step [432/433], Loss = 0.000798\n",
            "----------------------------------------\n",
            "Epoch [50/50], Step [108/433], Loss = 0.000135\n",
            "Epoch [50/50], Step [216/433], Loss = 0.000191\n",
            "Epoch [50/50], Step [324/433], Loss = 0.000531\n",
            "Epoch [50/50], Step [432/433], Loss = 0.053992\n",
            "----------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YEnJm_yzn57"
      },
      "source": [
        "# evaluating model and getting features of every image\r\n",
        "\r\n",
        "def eval_model_extract_features(features,true_labels,model,dataloader,phase):\r\n",
        "\r\n",
        "    with torch.no_grad():\r\n",
        "        # for entire dataset\r\n",
        "        n_correct = 0\r\n",
        "        n_samples = 0\r\n",
        "\r\n",
        "        model.eval()\r\n",
        "\r\n",
        "        for images,labels in dataloader:\r\n",
        "\r\n",
        "            images = images.to(device)\r\n",
        "            labels = labels.to(device)\r\n",
        "\r\n",
        "            true_labels.append(labels)\r\n",
        "            \r\n",
        "            ftrs,outputs = model(images)\r\n",
        "            features.append(ftrs)\r\n",
        "\r\n",
        "            _,preds = torch.max(outputs,1)\r\n",
        "            n_samples += labels.size(0)\r\n",
        "            n_correct += (preds == labels).sum().item()\r\n",
        "                \r\n",
        "        accuracy = n_correct/float(n_samples)\r\n",
        "\r\n",
        "        print(f'Accuracy of model on {phase} set = {(100.0 * accuracy):.4f} %')\r\n",
        "\r\n",
        "    return features,true_labels\r\n",
        "        "
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjJkrdVwGwmq"
      },
      "source": [
        "features = []\r\n",
        "true_labels = []"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a1leqfGHBKj",
        "outputId": "8b82a2d9-0808-4595-f2b1-27776867c119"
      },
      "source": [
        "train_loader = DataLoader(dataset=train_dataset,\r\n",
        "                         batch_size=1,\r\n",
        "                         shuffle=False,\r\n",
        "                         num_workers=4)\r\n",
        "\r\n",
        "features,true_labels = eval_model_extract_features(features,true_labels,model,dataloader=train_loader,phase='training')\r\n",
        "\r\n",
        "print(len(features),len(true_labels))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy of model on training set = 99.7687 %\n",
            "3458 3458\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoIhEXttPr7i",
        "outputId": "d145f4b4-6767-43b3-fdc5-030d138c6242"
      },
      "source": [
        "features,true_labels = eval_model_extract_features(features,true_labels,model,dataloader=val_loader,phase='validation')\r\n",
        "\r\n",
        "print(len(features),len(true_labels))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy of model on validation set = 94.3353 %\n",
            "4323 4323\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__066uQqzzOp"
      },
      "source": [
        "ftrs = features.copy() \r\n",
        "lbls = true_labels.copy()"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvD2OMtxJxmc",
        "outputId": "ce693e60-6205-4625-cc2a-9ea67295775a"
      },
      "source": [
        "for i in range(len(ftrs)):\r\n",
        "    ftrs[i]=ftrs[i].cpu().numpy()\r\n",
        "\r\n",
        "ftrs[0].shape"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 512)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJnf90G8Jxk1",
        "outputId": "63161742-dfe5-4f5b-b952-0cbc095059fe"
      },
      "source": [
        "for i in range(len(lbls)):\r\n",
        "    lbls[i]=lbls[i].cpu().numpy()\r\n",
        "\r\n",
        "lbls[0].shape"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omsThlPrRtG6",
        "outputId": "1d4bca18-c36b-4370-daa6-3ab9e591289c"
      },
      "source": [
        "type(ftrs),type(lbls)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(list, list)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riOhS9vdR41V",
        "outputId": "e7f7ff4c-cfff-411d-83aa-6776e3ec9092"
      },
      "source": [
        "ftrs = np.array(ftrs)\r\n",
        "lbls = np.array(lbls)\r\n",
        "\r\n",
        "ftrs.shape,lbls.shape"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4323, 1, 512), (4323, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2chM26KJSI6P",
        "outputId": "c3201c78-1536-48f8-8cd6-8367408e9a06"
      },
      "source": [
        "n_samples = ftrs.shape[0]*ftrs.shape[1]\r\n",
        "n_features = ftrs.shape[2]\r\n",
        "ftrs = ftrs.reshape(n_samples,n_features)\r\n",
        "\r\n",
        "print(ftrs.shape)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4323, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOrlzTMlSPSb",
        "outputId": "78e228f4-daec-4534-955a-dd85bd6672de"
      },
      "source": [
        "n_lbls = lbls.shape[0]\r\n",
        "lbls = lbls.reshape(n_lbls)\r\n",
        "\r\n",
        "print(lbls.shape)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4323,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "7R0c6-1iKPsO",
        "outputId": "8b545c97-2eb5-4953-fbd0-aeaa1aca9a83"
      },
      "source": [
        "# save to csv\r\n",
        "ftrs_df = pd.DataFrame(ftrs)\r\n",
        "ftrs_df.to_csv('./resnet50_FC_features_512.csv',index=False)\r\n",
        "\r\n",
        "# reloading the saved csv into a df\r\n",
        "\r\n",
        "ftrs_df = pd.read_csv('./resnet50_FC_features_512.csv')\r\n",
        "ftrs_df"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>472</th>\n",
              "      <th>473</th>\n",
              "      <th>474</th>\n",
              "      <th>475</th>\n",
              "      <th>476</th>\n",
              "      <th>477</th>\n",
              "      <th>478</th>\n",
              "      <th>479</th>\n",
              "      <th>480</th>\n",
              "      <th>481</th>\n",
              "      <th>482</th>\n",
              "      <th>483</th>\n",
              "      <th>484</th>\n",
              "      <th>485</th>\n",
              "      <th>486</th>\n",
              "      <th>487</th>\n",
              "      <th>488</th>\n",
              "      <th>489</th>\n",
              "      <th>490</th>\n",
              "      <th>491</th>\n",
              "      <th>492</th>\n",
              "      <th>493</th>\n",
              "      <th>494</th>\n",
              "      <th>495</th>\n",
              "      <th>496</th>\n",
              "      <th>497</th>\n",
              "      <th>498</th>\n",
              "      <th>499</th>\n",
              "      <th>500</th>\n",
              "      <th>501</th>\n",
              "      <th>502</th>\n",
              "      <th>503</th>\n",
              "      <th>504</th>\n",
              "      <th>505</th>\n",
              "      <th>506</th>\n",
              "      <th>507</th>\n",
              "      <th>508</th>\n",
              "      <th>509</th>\n",
              "      <th>510</th>\n",
              "      <th>511</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.085607</td>\n",
              "      <td>0.021805</td>\n",
              "      <td>0.441109</td>\n",
              "      <td>-0.286769</td>\n",
              "      <td>0.513648</td>\n",
              "      <td>-0.185310</td>\n",
              "      <td>-0.292494</td>\n",
              "      <td>-0.433075</td>\n",
              "      <td>-0.010477</td>\n",
              "      <td>-0.379597</td>\n",
              "      <td>-0.464587</td>\n",
              "      <td>-0.311266</td>\n",
              "      <td>-0.397218</td>\n",
              "      <td>-0.035729</td>\n",
              "      <td>-0.184875</td>\n",
              "      <td>0.005529</td>\n",
              "      <td>-0.485860</td>\n",
              "      <td>-0.084951</td>\n",
              "      <td>1.079289</td>\n",
              "      <td>0.234766</td>\n",
              "      <td>-0.607950</td>\n",
              "      <td>-0.416270</td>\n",
              "      <td>0.270703</td>\n",
              "      <td>0.078726</td>\n",
              "      <td>3.390669</td>\n",
              "      <td>1.175128</td>\n",
              "      <td>1.639136</td>\n",
              "      <td>0.919151</td>\n",
              "      <td>0.297039</td>\n",
              "      <td>1.392895</td>\n",
              "      <td>-0.078611</td>\n",
              "      <td>-0.245920</td>\n",
              "      <td>-0.380388</td>\n",
              "      <td>-1.181374</td>\n",
              "      <td>0.501518</td>\n",
              "      <td>-0.359335</td>\n",
              "      <td>0.949465</td>\n",
              "      <td>-0.596463</td>\n",
              "      <td>0.978446</td>\n",
              "      <td>0.250207</td>\n",
              "      <td>...</td>\n",
              "      <td>0.213398</td>\n",
              "      <td>-0.673682</td>\n",
              "      <td>0.134089</td>\n",
              "      <td>-0.830013</td>\n",
              "      <td>-0.270452</td>\n",
              "      <td>0.274271</td>\n",
              "      <td>-0.492283</td>\n",
              "      <td>1.312896</td>\n",
              "      <td>0.623881</td>\n",
              "      <td>-0.427828</td>\n",
              "      <td>-0.434984</td>\n",
              "      <td>-0.211153</td>\n",
              "      <td>0.655323</td>\n",
              "      <td>0.089081</td>\n",
              "      <td>2.196209</td>\n",
              "      <td>-0.643546</td>\n",
              "      <td>0.143208</td>\n",
              "      <td>1.332753</td>\n",
              "      <td>-0.046068</td>\n",
              "      <td>0.817310</td>\n",
              "      <td>0.190408</td>\n",
              "      <td>-0.307006</td>\n",
              "      <td>0.571560</td>\n",
              "      <td>0.143442</td>\n",
              "      <td>1.586519</td>\n",
              "      <td>0.291949</td>\n",
              "      <td>-0.515978</td>\n",
              "      <td>1.637968</td>\n",
              "      <td>1.365301</td>\n",
              "      <td>0.992635</td>\n",
              "      <td>1.900833</td>\n",
              "      <td>-0.517263</td>\n",
              "      <td>0.046018</td>\n",
              "      <td>-0.363306</td>\n",
              "      <td>-0.605235</td>\n",
              "      <td>-0.088705</td>\n",
              "      <td>0.874085</td>\n",
              "      <td>-0.474211</td>\n",
              "      <td>-0.151722</td>\n",
              "      <td>-0.569436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.133243</td>\n",
              "      <td>-0.964772</td>\n",
              "      <td>-0.479122</td>\n",
              "      <td>-0.388593</td>\n",
              "      <td>2.520442</td>\n",
              "      <td>-0.078540</td>\n",
              "      <td>1.397684</td>\n",
              "      <td>-0.962195</td>\n",
              "      <td>-0.197336</td>\n",
              "      <td>-0.283352</td>\n",
              "      <td>-0.368340</td>\n",
              "      <td>-0.348090</td>\n",
              "      <td>-0.860821</td>\n",
              "      <td>0.803020</td>\n",
              "      <td>-0.532486</td>\n",
              "      <td>-0.219943</td>\n",
              "      <td>-0.567403</td>\n",
              "      <td>-0.213607</td>\n",
              "      <td>1.202563</td>\n",
              "      <td>1.007898</td>\n",
              "      <td>1.166395</td>\n",
              "      <td>-0.908014</td>\n",
              "      <td>1.859558</td>\n",
              "      <td>1.871797</td>\n",
              "      <td>2.570776</td>\n",
              "      <td>1.382042</td>\n",
              "      <td>2.295932</td>\n",
              "      <td>0.721915</td>\n",
              "      <td>1.490825</td>\n",
              "      <td>0.843888</td>\n",
              "      <td>-0.023431</td>\n",
              "      <td>0.762509</td>\n",
              "      <td>-0.507451</td>\n",
              "      <td>0.378494</td>\n",
              "      <td>0.872305</td>\n",
              "      <td>-0.252291</td>\n",
              "      <td>2.568783</td>\n",
              "      <td>-1.072574</td>\n",
              "      <td>-0.142558</td>\n",
              "      <td>0.200490</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.020628</td>\n",
              "      <td>-0.657437</td>\n",
              "      <td>0.102203</td>\n",
              "      <td>-0.975201</td>\n",
              "      <td>-0.341460</td>\n",
              "      <td>1.338218</td>\n",
              "      <td>-0.867677</td>\n",
              "      <td>1.562890</td>\n",
              "      <td>-0.927345</td>\n",
              "      <td>-0.620522</td>\n",
              "      <td>-0.262975</td>\n",
              "      <td>-0.594635</td>\n",
              "      <td>2.326258</td>\n",
              "      <td>-0.029713</td>\n",
              "      <td>1.235405</td>\n",
              "      <td>-1.075083</td>\n",
              "      <td>-0.299171</td>\n",
              "      <td>1.695334</td>\n",
              "      <td>2.343036</td>\n",
              "      <td>0.426081</td>\n",
              "      <td>0.945150</td>\n",
              "      <td>-0.855601</td>\n",
              "      <td>0.465321</td>\n",
              "      <td>0.578151</td>\n",
              "      <td>2.881992</td>\n",
              "      <td>0.689831</td>\n",
              "      <td>-0.723939</td>\n",
              "      <td>0.376424</td>\n",
              "      <td>1.517536</td>\n",
              "      <td>-0.131685</td>\n",
              "      <td>0.042126</td>\n",
              "      <td>-0.536044</td>\n",
              "      <td>-0.029770</td>\n",
              "      <td>-0.425078</td>\n",
              "      <td>-0.667606</td>\n",
              "      <td>-0.453189</td>\n",
              "      <td>2.844271</td>\n",
              "      <td>-0.952448</td>\n",
              "      <td>1.436085</td>\n",
              "      <td>1.016485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.083105</td>\n",
              "      <td>-0.696422</td>\n",
              "      <td>-0.796738</td>\n",
              "      <td>-0.497731</td>\n",
              "      <td>1.963784</td>\n",
              "      <td>-0.284097</td>\n",
              "      <td>3.261419</td>\n",
              "      <td>-0.800969</td>\n",
              "      <td>-0.506469</td>\n",
              "      <td>-0.441910</td>\n",
              "      <td>0.079485</td>\n",
              "      <td>-0.463492</td>\n",
              "      <td>-0.471498</td>\n",
              "      <td>0.799019</td>\n",
              "      <td>-0.376445</td>\n",
              "      <td>-0.360156</td>\n",
              "      <td>-0.686314</td>\n",
              "      <td>-0.172683</td>\n",
              "      <td>1.956364</td>\n",
              "      <td>-0.891847</td>\n",
              "      <td>3.087634</td>\n",
              "      <td>-0.818431</td>\n",
              "      <td>0.335942</td>\n",
              "      <td>0.611970</td>\n",
              "      <td>0.758249</td>\n",
              "      <td>1.121906</td>\n",
              "      <td>0.802602</td>\n",
              "      <td>0.901728</td>\n",
              "      <td>3.559395</td>\n",
              "      <td>-1.312710</td>\n",
              "      <td>0.931835</td>\n",
              "      <td>2.984989</td>\n",
              "      <td>-0.468388</td>\n",
              "      <td>1.456828</td>\n",
              "      <td>0.267345</td>\n",
              "      <td>-0.318779</td>\n",
              "      <td>3.901358</td>\n",
              "      <td>-0.609226</td>\n",
              "      <td>-0.471357</td>\n",
              "      <td>1.560983</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.130084</td>\n",
              "      <td>-0.486394</td>\n",
              "      <td>0.011836</td>\n",
              "      <td>-0.607265</td>\n",
              "      <td>-0.756973</td>\n",
              "      <td>0.369215</td>\n",
              "      <td>-0.849208</td>\n",
              "      <td>2.028156</td>\n",
              "      <td>0.119567</td>\n",
              "      <td>-0.803319</td>\n",
              "      <td>-0.465819</td>\n",
              "      <td>0.024541</td>\n",
              "      <td>0.522723</td>\n",
              "      <td>-0.393713</td>\n",
              "      <td>0.456446</td>\n",
              "      <td>-1.160259</td>\n",
              "      <td>-0.269853</td>\n",
              "      <td>1.397479</td>\n",
              "      <td>0.577923</td>\n",
              "      <td>-0.016859</td>\n",
              "      <td>1.445956</td>\n",
              "      <td>-0.849506</td>\n",
              "      <td>2.242668</td>\n",
              "      <td>1.023136</td>\n",
              "      <td>1.804294</td>\n",
              "      <td>0.145638</td>\n",
              "      <td>-0.650150</td>\n",
              "      <td>1.232863</td>\n",
              "      <td>2.362357</td>\n",
              "      <td>0.294656</td>\n",
              "      <td>0.325273</td>\n",
              "      <td>-0.361375</td>\n",
              "      <td>0.469979</td>\n",
              "      <td>-0.755104</td>\n",
              "      <td>-0.431179</td>\n",
              "      <td>0.206783</td>\n",
              "      <td>2.577395</td>\n",
              "      <td>-0.640026</td>\n",
              "      <td>0.363521</td>\n",
              "      <td>0.624068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.071460</td>\n",
              "      <td>-0.553484</td>\n",
              "      <td>0.029384</td>\n",
              "      <td>-0.145030</td>\n",
              "      <td>-0.509019</td>\n",
              "      <td>-0.443575</td>\n",
              "      <td>2.058645</td>\n",
              "      <td>-0.614785</td>\n",
              "      <td>-0.025296</td>\n",
              "      <td>-0.196204</td>\n",
              "      <td>1.508216</td>\n",
              "      <td>-0.386372</td>\n",
              "      <td>-0.322134</td>\n",
              "      <td>0.093845</td>\n",
              "      <td>-0.164296</td>\n",
              "      <td>-0.202908</td>\n",
              "      <td>-0.675448</td>\n",
              "      <td>1.987206</td>\n",
              "      <td>0.743067</td>\n",
              "      <td>-0.112193</td>\n",
              "      <td>1.952012</td>\n",
              "      <td>-0.279601</td>\n",
              "      <td>1.317924</td>\n",
              "      <td>1.228551</td>\n",
              "      <td>-0.335249</td>\n",
              "      <td>-0.084516</td>\n",
              "      <td>0.756525</td>\n",
              "      <td>1.279888</td>\n",
              "      <td>0.214162</td>\n",
              "      <td>-0.109619</td>\n",
              "      <td>1.804547</td>\n",
              "      <td>1.737462</td>\n",
              "      <td>-0.252732</td>\n",
              "      <td>1.941269</td>\n",
              "      <td>-0.171081</td>\n",
              "      <td>-0.347327</td>\n",
              "      <td>-0.324948</td>\n",
              "      <td>-0.587768</td>\n",
              "      <td>0.029561</td>\n",
              "      <td>-0.269150</td>\n",
              "      <td>...</td>\n",
              "      <td>0.598750</td>\n",
              "      <td>1.275778</td>\n",
              "      <td>0.271461</td>\n",
              "      <td>2.004199</td>\n",
              "      <td>-0.408011</td>\n",
              "      <td>1.705920</td>\n",
              "      <td>-0.323414</td>\n",
              "      <td>-0.148019</td>\n",
              "      <td>0.805807</td>\n",
              "      <td>-0.330228</td>\n",
              "      <td>-0.294776</td>\n",
              "      <td>0.718698</td>\n",
              "      <td>0.251789</td>\n",
              "      <td>0.782802</td>\n",
              "      <td>-0.270511</td>\n",
              "      <td>-0.356032</td>\n",
              "      <td>-0.055300</td>\n",
              "      <td>0.247848</td>\n",
              "      <td>1.221817</td>\n",
              "      <td>-0.020865</td>\n",
              "      <td>1.573320</td>\n",
              "      <td>0.149294</td>\n",
              "      <td>-0.046459</td>\n",
              "      <td>0.353816</td>\n",
              "      <td>-0.122537</td>\n",
              "      <td>-0.027578</td>\n",
              "      <td>-0.480010</td>\n",
              "      <td>-0.243383</td>\n",
              "      <td>-0.362624</td>\n",
              "      <td>0.525934</td>\n",
              "      <td>-0.071147</td>\n",
              "      <td>-0.662112</td>\n",
              "      <td>-0.019017</td>\n",
              "      <td>-0.365846</td>\n",
              "      <td>-0.136268</td>\n",
              "      <td>0.428925</td>\n",
              "      <td>-0.099518</td>\n",
              "      <td>-0.303276</td>\n",
              "      <td>2.082908</td>\n",
              "      <td>0.965919</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.197739</td>\n",
              "      <td>-0.731074</td>\n",
              "      <td>-0.256347</td>\n",
              "      <td>-1.116051</td>\n",
              "      <td>-1.104760</td>\n",
              "      <td>0.012868</td>\n",
              "      <td>0.499145</td>\n",
              "      <td>-0.610310</td>\n",
              "      <td>-0.062483</td>\n",
              "      <td>-0.172752</td>\n",
              "      <td>-0.808338</td>\n",
              "      <td>-0.300046</td>\n",
              "      <td>-1.091365</td>\n",
              "      <td>-0.175848</td>\n",
              "      <td>-0.590491</td>\n",
              "      <td>-0.383289</td>\n",
              "      <td>-0.950313</td>\n",
              "      <td>1.537892</td>\n",
              "      <td>2.285882</td>\n",
              "      <td>-0.894574</td>\n",
              "      <td>1.005046</td>\n",
              "      <td>-0.548572</td>\n",
              "      <td>0.915498</td>\n",
              "      <td>2.209650</td>\n",
              "      <td>4.332871</td>\n",
              "      <td>1.777773</td>\n",
              "      <td>3.032871</td>\n",
              "      <td>2.438895</td>\n",
              "      <td>0.461631</td>\n",
              "      <td>1.467396</td>\n",
              "      <td>0.488518</td>\n",
              "      <td>0.534241</td>\n",
              "      <td>-0.472278</td>\n",
              "      <td>0.219244</td>\n",
              "      <td>0.072697</td>\n",
              "      <td>-0.203950</td>\n",
              "      <td>-0.583803</td>\n",
              "      <td>-0.649506</td>\n",
              "      <td>1.389083</td>\n",
              "      <td>-0.124601</td>\n",
              "      <td>...</td>\n",
              "      <td>0.184684</td>\n",
              "      <td>-0.562442</td>\n",
              "      <td>0.392000</td>\n",
              "      <td>0.149472</td>\n",
              "      <td>-0.060157</td>\n",
              "      <td>2.243619</td>\n",
              "      <td>-0.548696</td>\n",
              "      <td>1.862436</td>\n",
              "      <td>0.320670</td>\n",
              "      <td>-0.460364</td>\n",
              "      <td>-0.493218</td>\n",
              "      <td>-1.026154</td>\n",
              "      <td>0.948146</td>\n",
              "      <td>1.560519</td>\n",
              "      <td>2.732547</td>\n",
              "      <td>-1.321941</td>\n",
              "      <td>-0.210493</td>\n",
              "      <td>2.269151</td>\n",
              "      <td>1.818774</td>\n",
              "      <td>-0.473236</td>\n",
              "      <td>2.870932</td>\n",
              "      <td>-1.494021</td>\n",
              "      <td>-0.124955</td>\n",
              "      <td>0.283038</td>\n",
              "      <td>2.440798</td>\n",
              "      <td>1.405221</td>\n",
              "      <td>-1.211263</td>\n",
              "      <td>1.504962</td>\n",
              "      <td>0.168726</td>\n",
              "      <td>0.873936</td>\n",
              "      <td>2.443866</td>\n",
              "      <td>-0.567604</td>\n",
              "      <td>-0.187445</td>\n",
              "      <td>-0.365194</td>\n",
              "      <td>-0.753977</td>\n",
              "      <td>-0.016396</td>\n",
              "      <td>0.172039</td>\n",
              "      <td>-0.294501</td>\n",
              "      <td>0.093440</td>\n",
              "      <td>0.641316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4318</th>\n",
              "      <td>-0.411964</td>\n",
              "      <td>-0.215166</td>\n",
              "      <td>0.397792</td>\n",
              "      <td>0.127926</td>\n",
              "      <td>0.365831</td>\n",
              "      <td>-0.639052</td>\n",
              "      <td>1.061767</td>\n",
              "      <td>-0.250366</td>\n",
              "      <td>-0.247163</td>\n",
              "      <td>-0.370309</td>\n",
              "      <td>1.497652</td>\n",
              "      <td>-0.214139</td>\n",
              "      <td>-0.195388</td>\n",
              "      <td>-0.013514</td>\n",
              "      <td>0.012661</td>\n",
              "      <td>0.188817</td>\n",
              "      <td>-0.336080</td>\n",
              "      <td>1.103943</td>\n",
              "      <td>-0.164219</td>\n",
              "      <td>1.557188</td>\n",
              "      <td>0.655723</td>\n",
              "      <td>-0.521478</td>\n",
              "      <td>1.169153</td>\n",
              "      <td>0.040593</td>\n",
              "      <td>-0.518317</td>\n",
              "      <td>-0.471488</td>\n",
              "      <td>0.012427</td>\n",
              "      <td>0.054357</td>\n",
              "      <td>-0.207444</td>\n",
              "      <td>1.032838</td>\n",
              "      <td>1.547008</td>\n",
              "      <td>0.688899</td>\n",
              "      <td>-0.316939</td>\n",
              "      <td>0.648589</td>\n",
              "      <td>0.135564</td>\n",
              "      <td>-0.549887</td>\n",
              "      <td>-0.142928</td>\n",
              "      <td>-0.493280</td>\n",
              "      <td>-0.090844</td>\n",
              "      <td>-0.563234</td>\n",
              "      <td>...</td>\n",
              "      <td>0.748472</td>\n",
              "      <td>1.338823</td>\n",
              "      <td>0.153386</td>\n",
              "      <td>1.769272</td>\n",
              "      <td>-0.446314</td>\n",
              "      <td>0.650508</td>\n",
              "      <td>-0.419870</td>\n",
              "      <td>-0.274930</td>\n",
              "      <td>0.959066</td>\n",
              "      <td>-0.248627</td>\n",
              "      <td>-0.318770</td>\n",
              "      <td>1.104606</td>\n",
              "      <td>0.445158</td>\n",
              "      <td>0.750738</td>\n",
              "      <td>-0.376530</td>\n",
              "      <td>0.280118</td>\n",
              "      <td>0.080847</td>\n",
              "      <td>-0.069336</td>\n",
              "      <td>0.432121</td>\n",
              "      <td>0.767615</td>\n",
              "      <td>0.103846</td>\n",
              "      <td>0.989550</td>\n",
              "      <td>-0.475103</td>\n",
              "      <td>-0.104094</td>\n",
              "      <td>-0.102964</td>\n",
              "      <td>0.004872</td>\n",
              "      <td>-0.392388</td>\n",
              "      <td>-0.187689</td>\n",
              "      <td>0.181535</td>\n",
              "      <td>0.615621</td>\n",
              "      <td>-0.114013</td>\n",
              "      <td>-0.522579</td>\n",
              "      <td>0.209361</td>\n",
              "      <td>-0.475251</td>\n",
              "      <td>0.033660</td>\n",
              "      <td>-0.036103</td>\n",
              "      <td>-0.219503</td>\n",
              "      <td>-0.180813</td>\n",
              "      <td>2.115536</td>\n",
              "      <td>0.385495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4319</th>\n",
              "      <td>-0.744634</td>\n",
              "      <td>-0.367143</td>\n",
              "      <td>1.707262</td>\n",
              "      <td>0.563260</td>\n",
              "      <td>0.069633</td>\n",
              "      <td>-0.691620</td>\n",
              "      <td>0.129602</td>\n",
              "      <td>-0.526031</td>\n",
              "      <td>-0.377778</td>\n",
              "      <td>-0.736090</td>\n",
              "      <td>2.004293</td>\n",
              "      <td>-0.408265</td>\n",
              "      <td>-0.393246</td>\n",
              "      <td>-0.356447</td>\n",
              "      <td>0.076957</td>\n",
              "      <td>0.251363</td>\n",
              "      <td>-0.789603</td>\n",
              "      <td>1.927533</td>\n",
              "      <td>-0.053897</td>\n",
              "      <td>2.110189</td>\n",
              "      <td>-0.328533</td>\n",
              "      <td>-0.817039</td>\n",
              "      <td>1.364293</td>\n",
              "      <td>-0.739697</td>\n",
              "      <td>0.126688</td>\n",
              "      <td>0.165101</td>\n",
              "      <td>-0.221024</td>\n",
              "      <td>0.270788</td>\n",
              "      <td>-0.458646</td>\n",
              "      <td>2.639428</td>\n",
              "      <td>2.171700</td>\n",
              "      <td>-0.067937</td>\n",
              "      <td>-0.599245</td>\n",
              "      <td>-0.461420</td>\n",
              "      <td>0.408699</td>\n",
              "      <td>-1.055864</td>\n",
              "      <td>-0.621415</td>\n",
              "      <td>-0.718658</td>\n",
              "      <td>0.534098</td>\n",
              "      <td>-0.680208</td>\n",
              "      <td>...</td>\n",
              "      <td>1.283679</td>\n",
              "      <td>2.375924</td>\n",
              "      <td>-0.003406</td>\n",
              "      <td>1.893804</td>\n",
              "      <td>-0.873891</td>\n",
              "      <td>0.238221</td>\n",
              "      <td>-0.773082</td>\n",
              "      <td>-0.402667</td>\n",
              "      <td>2.376142</td>\n",
              "      <td>-0.437616</td>\n",
              "      <td>-0.652478</td>\n",
              "      <td>2.314448</td>\n",
              "      <td>1.110072</td>\n",
              "      <td>1.417839</td>\n",
              "      <td>0.516021</td>\n",
              "      <td>-0.226597</td>\n",
              "      <td>0.085850</td>\n",
              "      <td>0.059442</td>\n",
              "      <td>-0.809649</td>\n",
              "      <td>1.077470</td>\n",
              "      <td>-0.256480</td>\n",
              "      <td>2.136263</td>\n",
              "      <td>-0.387530</td>\n",
              "      <td>-0.117997</td>\n",
              "      <td>-0.276213</td>\n",
              "      <td>-0.259430</td>\n",
              "      <td>-1.149565</td>\n",
              "      <td>0.687346</td>\n",
              "      <td>-0.313412</td>\n",
              "      <td>1.807125</td>\n",
              "      <td>1.162865</td>\n",
              "      <td>-0.855010</td>\n",
              "      <td>0.792749</td>\n",
              "      <td>-0.682653</td>\n",
              "      <td>-0.344341</td>\n",
              "      <td>-0.449044</td>\n",
              "      <td>-0.725507</td>\n",
              "      <td>-0.555504</td>\n",
              "      <td>2.674017</td>\n",
              "      <td>-0.772274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4320</th>\n",
              "      <td>-0.189872</td>\n",
              "      <td>-0.064006</td>\n",
              "      <td>0.198464</td>\n",
              "      <td>0.052106</td>\n",
              "      <td>-0.556809</td>\n",
              "      <td>-0.325992</td>\n",
              "      <td>1.246096</td>\n",
              "      <td>-0.125790</td>\n",
              "      <td>-0.203488</td>\n",
              "      <td>-0.040403</td>\n",
              "      <td>0.882528</td>\n",
              "      <td>-0.256750</td>\n",
              "      <td>-0.152355</td>\n",
              "      <td>-0.164828</td>\n",
              "      <td>-0.004779</td>\n",
              "      <td>0.014405</td>\n",
              "      <td>-0.113635</td>\n",
              "      <td>0.965828</td>\n",
              "      <td>0.237597</td>\n",
              "      <td>-0.088504</td>\n",
              "      <td>1.087871</td>\n",
              "      <td>-0.319717</td>\n",
              "      <td>0.501730</td>\n",
              "      <td>0.358327</td>\n",
              "      <td>-0.670823</td>\n",
              "      <td>-0.460788</td>\n",
              "      <td>-0.022841</td>\n",
              "      <td>0.590250</td>\n",
              "      <td>-0.073192</td>\n",
              "      <td>-0.358912</td>\n",
              "      <td>1.104351</td>\n",
              "      <td>1.078326</td>\n",
              "      <td>-0.239340</td>\n",
              "      <td>1.085713</td>\n",
              "      <td>-0.188321</td>\n",
              "      <td>-0.453035</td>\n",
              "      <td>-0.470226</td>\n",
              "      <td>-0.425583</td>\n",
              "      <td>0.210761</td>\n",
              "      <td>-0.221895</td>\n",
              "      <td>...</td>\n",
              "      <td>0.608380</td>\n",
              "      <td>1.263205</td>\n",
              "      <td>0.002098</td>\n",
              "      <td>1.483538</td>\n",
              "      <td>-0.377799</td>\n",
              "      <td>0.971156</td>\n",
              "      <td>-0.218028</td>\n",
              "      <td>-0.211596</td>\n",
              "      <td>0.677350</td>\n",
              "      <td>-0.143152</td>\n",
              "      <td>-0.142737</td>\n",
              "      <td>0.839846</td>\n",
              "      <td>-0.201976</td>\n",
              "      <td>0.756051</td>\n",
              "      <td>-0.535939</td>\n",
              "      <td>-0.116270</td>\n",
              "      <td>-0.175716</td>\n",
              "      <td>-0.104628</td>\n",
              "      <td>0.470687</td>\n",
              "      <td>0.122938</td>\n",
              "      <td>0.774564</td>\n",
              "      <td>0.282786</td>\n",
              "      <td>-0.140790</td>\n",
              "      <td>0.116737</td>\n",
              "      <td>-0.369623</td>\n",
              "      <td>-0.191232</td>\n",
              "      <td>-0.398529</td>\n",
              "      <td>-0.119287</td>\n",
              "      <td>-0.540150</td>\n",
              "      <td>0.332821</td>\n",
              "      <td>-0.215005</td>\n",
              "      <td>-0.370967</td>\n",
              "      <td>0.168773</td>\n",
              "      <td>-0.028186</td>\n",
              "      <td>-0.061796</td>\n",
              "      <td>0.390675</td>\n",
              "      <td>-0.531841</td>\n",
              "      <td>-0.157892</td>\n",
              "      <td>1.204072</td>\n",
              "      <td>0.653930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4321</th>\n",
              "      <td>0.197524</td>\n",
              "      <td>-0.386280</td>\n",
              "      <td>0.245068</td>\n",
              "      <td>-0.459268</td>\n",
              "      <td>-0.636512</td>\n",
              "      <td>-0.162679</td>\n",
              "      <td>0.479350</td>\n",
              "      <td>-0.480562</td>\n",
              "      <td>-0.025279</td>\n",
              "      <td>-0.069157</td>\n",
              "      <td>-0.346472</td>\n",
              "      <td>-0.249377</td>\n",
              "      <td>-0.406491</td>\n",
              "      <td>0.128068</td>\n",
              "      <td>-0.299416</td>\n",
              "      <td>-0.246644</td>\n",
              "      <td>-0.377301</td>\n",
              "      <td>0.904717</td>\n",
              "      <td>1.483523</td>\n",
              "      <td>-0.725433</td>\n",
              "      <td>0.873144</td>\n",
              "      <td>-0.354503</td>\n",
              "      <td>0.134507</td>\n",
              "      <td>0.702128</td>\n",
              "      <td>2.022075</td>\n",
              "      <td>1.200273</td>\n",
              "      <td>1.347055</td>\n",
              "      <td>1.335216</td>\n",
              "      <td>0.391315</td>\n",
              "      <td>0.567601</td>\n",
              "      <td>0.579339</td>\n",
              "      <td>0.578097</td>\n",
              "      <td>-0.335478</td>\n",
              "      <td>0.249294</td>\n",
              "      <td>-0.303445</td>\n",
              "      <td>-0.234489</td>\n",
              "      <td>0.046308</td>\n",
              "      <td>-0.314374</td>\n",
              "      <td>0.911541</td>\n",
              "      <td>0.213348</td>\n",
              "      <td>...</td>\n",
              "      <td>0.144132</td>\n",
              "      <td>-0.193557</td>\n",
              "      <td>0.294394</td>\n",
              "      <td>0.080515</td>\n",
              "      <td>-0.294245</td>\n",
              "      <td>0.775613</td>\n",
              "      <td>-0.349679</td>\n",
              "      <td>1.201547</td>\n",
              "      <td>0.641203</td>\n",
              "      <td>-0.232427</td>\n",
              "      <td>-0.415436</td>\n",
              "      <td>-0.333580</td>\n",
              "      <td>0.138332</td>\n",
              "      <td>0.737065</td>\n",
              "      <td>1.390217</td>\n",
              "      <td>-0.603217</td>\n",
              "      <td>0.101477</td>\n",
              "      <td>1.131908</td>\n",
              "      <td>0.365596</td>\n",
              "      <td>-0.368327</td>\n",
              "      <td>1.525597</td>\n",
              "      <td>-0.404978</td>\n",
              "      <td>0.468510</td>\n",
              "      <td>0.068937</td>\n",
              "      <td>1.040590</td>\n",
              "      <td>0.312121</td>\n",
              "      <td>-0.710542</td>\n",
              "      <td>1.058605</td>\n",
              "      <td>0.103527</td>\n",
              "      <td>0.811860</td>\n",
              "      <td>1.489462</td>\n",
              "      <td>-0.398147</td>\n",
              "      <td>0.020573</td>\n",
              "      <td>-0.133686</td>\n",
              "      <td>-0.239739</td>\n",
              "      <td>0.008940</td>\n",
              "      <td>-0.189435</td>\n",
              "      <td>-0.142793</td>\n",
              "      <td>-0.277332</td>\n",
              "      <td>0.049284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4322</th>\n",
              "      <td>-0.434214</td>\n",
              "      <td>-0.593702</td>\n",
              "      <td>0.279713</td>\n",
              "      <td>-0.165080</td>\n",
              "      <td>1.136062</td>\n",
              "      <td>-0.667113</td>\n",
              "      <td>2.052018</td>\n",
              "      <td>-0.604946</td>\n",
              "      <td>-0.234777</td>\n",
              "      <td>-0.452047</td>\n",
              "      <td>1.708738</td>\n",
              "      <td>-0.440759</td>\n",
              "      <td>-0.295052</td>\n",
              "      <td>0.379277</td>\n",
              "      <td>-0.020354</td>\n",
              "      <td>0.211516</td>\n",
              "      <td>-0.571302</td>\n",
              "      <td>0.743255</td>\n",
              "      <td>0.361388</td>\n",
              "      <td>0.861009</td>\n",
              "      <td>1.516168</td>\n",
              "      <td>-0.835628</td>\n",
              "      <td>0.740826</td>\n",
              "      <td>-0.825328</td>\n",
              "      <td>-1.705146</td>\n",
              "      <td>-0.215304</td>\n",
              "      <td>-0.673148</td>\n",
              "      <td>0.286651</td>\n",
              "      <td>1.497001</td>\n",
              "      <td>0.185283</td>\n",
              "      <td>2.002091</td>\n",
              "      <td>1.704771</td>\n",
              "      <td>-0.376189</td>\n",
              "      <td>0.736344</td>\n",
              "      <td>0.262215</td>\n",
              "      <td>-0.699214</td>\n",
              "      <td>1.768105</td>\n",
              "      <td>-0.387274</td>\n",
              "      <td>-0.534823</td>\n",
              "      <td>0.577002</td>\n",
              "      <td>...</td>\n",
              "      <td>0.624190</td>\n",
              "      <td>1.626767</td>\n",
              "      <td>0.297837</td>\n",
              "      <td>1.095364</td>\n",
              "      <td>-0.643627</td>\n",
              "      <td>-0.274788</td>\n",
              "      <td>-0.662378</td>\n",
              "      <td>0.517036</td>\n",
              "      <td>1.933796</td>\n",
              "      <td>-0.660498</td>\n",
              "      <td>-0.487205</td>\n",
              "      <td>1.820283</td>\n",
              "      <td>0.444199</td>\n",
              "      <td>0.146028</td>\n",
              "      <td>-0.604062</td>\n",
              "      <td>-0.396953</td>\n",
              "      <td>-0.123701</td>\n",
              "      <td>-0.071921</td>\n",
              "      <td>-0.716614</td>\n",
              "      <td>0.756776</td>\n",
              "      <td>-0.017450</td>\n",
              "      <td>0.751152</td>\n",
              "      <td>1.032276</td>\n",
              "      <td>0.515174</td>\n",
              "      <td>-0.193282</td>\n",
              "      <td>-0.162689</td>\n",
              "      <td>-0.664880</td>\n",
              "      <td>0.706234</td>\n",
              "      <td>1.393164</td>\n",
              "      <td>1.089688</td>\n",
              "      <td>0.134665</td>\n",
              "      <td>-0.524124</td>\n",
              "      <td>0.685812</td>\n",
              "      <td>-0.636059</td>\n",
              "      <td>-0.159715</td>\n",
              "      <td>-0.188474</td>\n",
              "      <td>1.029296</td>\n",
              "      <td>-0.376084</td>\n",
              "      <td>1.563824</td>\n",
              "      <td>-0.422949</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4323 rows  512 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             0         1         2  ...       509       510       511\n",
              "0     0.085607  0.021805  0.441109  ... -0.474211 -0.151722 -0.569436\n",
              "1    -0.133243 -0.964772 -0.479122  ... -0.952448  1.436085  1.016485\n",
              "2     0.083105 -0.696422 -0.796738  ... -0.640026  0.363521  0.624068\n",
              "3    -0.071460 -0.553484  0.029384  ... -0.303276  2.082908  0.965919\n",
              "4     0.197739 -0.731074 -0.256347  ... -0.294501  0.093440  0.641316\n",
              "...        ...       ...       ...  ...       ...       ...       ...\n",
              "4318 -0.411964 -0.215166  0.397792  ... -0.180813  2.115536  0.385495\n",
              "4319 -0.744634 -0.367143  1.707262  ... -0.555504  2.674017 -0.772274\n",
              "4320 -0.189872 -0.064006  0.198464  ... -0.157892  1.204072  0.653930\n",
              "4321  0.197524 -0.386280  0.245068  ... -0.142793 -0.277332  0.049284\n",
              "4322 -0.434214 -0.593702  0.279713  ... -0.376084  1.563824 -0.422949\n",
              "\n",
              "[4323 rows x 512 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "T01wsyUKS9b8",
        "outputId": "ddbddf28-81e2-4c55-8185-108f5098a222"
      },
      "source": [
        "# appending labels to the feature set\r\n",
        "ftrs_df['label'] = lbls\r\n",
        "\r\n",
        "ftrs_df.head()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>473</th>\n",
              "      <th>474</th>\n",
              "      <th>475</th>\n",
              "      <th>476</th>\n",
              "      <th>477</th>\n",
              "      <th>478</th>\n",
              "      <th>479</th>\n",
              "      <th>480</th>\n",
              "      <th>481</th>\n",
              "      <th>482</th>\n",
              "      <th>483</th>\n",
              "      <th>484</th>\n",
              "      <th>485</th>\n",
              "      <th>486</th>\n",
              "      <th>487</th>\n",
              "      <th>488</th>\n",
              "      <th>489</th>\n",
              "      <th>490</th>\n",
              "      <th>491</th>\n",
              "      <th>492</th>\n",
              "      <th>493</th>\n",
              "      <th>494</th>\n",
              "      <th>495</th>\n",
              "      <th>496</th>\n",
              "      <th>497</th>\n",
              "      <th>498</th>\n",
              "      <th>499</th>\n",
              "      <th>500</th>\n",
              "      <th>501</th>\n",
              "      <th>502</th>\n",
              "      <th>503</th>\n",
              "      <th>504</th>\n",
              "      <th>505</th>\n",
              "      <th>506</th>\n",
              "      <th>507</th>\n",
              "      <th>508</th>\n",
              "      <th>509</th>\n",
              "      <th>510</th>\n",
              "      <th>511</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.085607</td>\n",
              "      <td>0.021805</td>\n",
              "      <td>0.441109</td>\n",
              "      <td>-0.286769</td>\n",
              "      <td>0.513648</td>\n",
              "      <td>-0.185310</td>\n",
              "      <td>-0.292494</td>\n",
              "      <td>-0.433075</td>\n",
              "      <td>-0.010477</td>\n",
              "      <td>-0.379597</td>\n",
              "      <td>-0.464587</td>\n",
              "      <td>-0.311266</td>\n",
              "      <td>-0.397218</td>\n",
              "      <td>-0.035729</td>\n",
              "      <td>-0.184875</td>\n",
              "      <td>0.005529</td>\n",
              "      <td>-0.485860</td>\n",
              "      <td>-0.084951</td>\n",
              "      <td>1.079289</td>\n",
              "      <td>0.234766</td>\n",
              "      <td>-0.607950</td>\n",
              "      <td>-0.416270</td>\n",
              "      <td>0.270703</td>\n",
              "      <td>0.078726</td>\n",
              "      <td>3.390669</td>\n",
              "      <td>1.175128</td>\n",
              "      <td>1.639136</td>\n",
              "      <td>0.919151</td>\n",
              "      <td>0.297039</td>\n",
              "      <td>1.392895</td>\n",
              "      <td>-0.078611</td>\n",
              "      <td>-0.245920</td>\n",
              "      <td>-0.380388</td>\n",
              "      <td>-1.181374</td>\n",
              "      <td>0.501518</td>\n",
              "      <td>-0.359335</td>\n",
              "      <td>0.949465</td>\n",
              "      <td>-0.596463</td>\n",
              "      <td>0.978446</td>\n",
              "      <td>0.250207</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.673682</td>\n",
              "      <td>0.134089</td>\n",
              "      <td>-0.830013</td>\n",
              "      <td>-0.270452</td>\n",
              "      <td>0.274271</td>\n",
              "      <td>-0.492283</td>\n",
              "      <td>1.312896</td>\n",
              "      <td>0.623881</td>\n",
              "      <td>-0.427828</td>\n",
              "      <td>-0.434984</td>\n",
              "      <td>-0.211153</td>\n",
              "      <td>0.655323</td>\n",
              "      <td>0.089081</td>\n",
              "      <td>2.196209</td>\n",
              "      <td>-0.643546</td>\n",
              "      <td>0.143208</td>\n",
              "      <td>1.332753</td>\n",
              "      <td>-0.046068</td>\n",
              "      <td>0.817310</td>\n",
              "      <td>0.190408</td>\n",
              "      <td>-0.307006</td>\n",
              "      <td>0.571560</td>\n",
              "      <td>0.143442</td>\n",
              "      <td>1.586519</td>\n",
              "      <td>0.291949</td>\n",
              "      <td>-0.515978</td>\n",
              "      <td>1.637968</td>\n",
              "      <td>1.365301</td>\n",
              "      <td>0.992635</td>\n",
              "      <td>1.900833</td>\n",
              "      <td>-0.517263</td>\n",
              "      <td>0.046018</td>\n",
              "      <td>-0.363306</td>\n",
              "      <td>-0.605235</td>\n",
              "      <td>-0.088705</td>\n",
              "      <td>0.874085</td>\n",
              "      <td>-0.474211</td>\n",
              "      <td>-0.151722</td>\n",
              "      <td>-0.569436</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.133243</td>\n",
              "      <td>-0.964772</td>\n",
              "      <td>-0.479122</td>\n",
              "      <td>-0.388593</td>\n",
              "      <td>2.520442</td>\n",
              "      <td>-0.078540</td>\n",
              "      <td>1.397684</td>\n",
              "      <td>-0.962195</td>\n",
              "      <td>-0.197336</td>\n",
              "      <td>-0.283352</td>\n",
              "      <td>-0.368340</td>\n",
              "      <td>-0.348090</td>\n",
              "      <td>-0.860821</td>\n",
              "      <td>0.803020</td>\n",
              "      <td>-0.532486</td>\n",
              "      <td>-0.219943</td>\n",
              "      <td>-0.567403</td>\n",
              "      <td>-0.213607</td>\n",
              "      <td>1.202563</td>\n",
              "      <td>1.007898</td>\n",
              "      <td>1.166395</td>\n",
              "      <td>-0.908014</td>\n",
              "      <td>1.859558</td>\n",
              "      <td>1.871797</td>\n",
              "      <td>2.570776</td>\n",
              "      <td>1.382042</td>\n",
              "      <td>2.295932</td>\n",
              "      <td>0.721915</td>\n",
              "      <td>1.490825</td>\n",
              "      <td>0.843888</td>\n",
              "      <td>-0.023431</td>\n",
              "      <td>0.762509</td>\n",
              "      <td>-0.507451</td>\n",
              "      <td>0.378494</td>\n",
              "      <td>0.872305</td>\n",
              "      <td>-0.252291</td>\n",
              "      <td>2.568783</td>\n",
              "      <td>-1.072574</td>\n",
              "      <td>-0.142558</td>\n",
              "      <td>0.200490</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.657437</td>\n",
              "      <td>0.102203</td>\n",
              "      <td>-0.975201</td>\n",
              "      <td>-0.341460</td>\n",
              "      <td>1.338218</td>\n",
              "      <td>-0.867677</td>\n",
              "      <td>1.562890</td>\n",
              "      <td>-0.927345</td>\n",
              "      <td>-0.620522</td>\n",
              "      <td>-0.262975</td>\n",
              "      <td>-0.594635</td>\n",
              "      <td>2.326258</td>\n",
              "      <td>-0.029713</td>\n",
              "      <td>1.235405</td>\n",
              "      <td>-1.075083</td>\n",
              "      <td>-0.299171</td>\n",
              "      <td>1.695334</td>\n",
              "      <td>2.343036</td>\n",
              "      <td>0.426081</td>\n",
              "      <td>0.945150</td>\n",
              "      <td>-0.855601</td>\n",
              "      <td>0.465321</td>\n",
              "      <td>0.578151</td>\n",
              "      <td>2.881992</td>\n",
              "      <td>0.689831</td>\n",
              "      <td>-0.723939</td>\n",
              "      <td>0.376424</td>\n",
              "      <td>1.517536</td>\n",
              "      <td>-0.131685</td>\n",
              "      <td>0.042126</td>\n",
              "      <td>-0.536044</td>\n",
              "      <td>-0.029770</td>\n",
              "      <td>-0.425078</td>\n",
              "      <td>-0.667606</td>\n",
              "      <td>-0.453189</td>\n",
              "      <td>2.844271</td>\n",
              "      <td>-0.952448</td>\n",
              "      <td>1.436085</td>\n",
              "      <td>1.016485</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.083105</td>\n",
              "      <td>-0.696422</td>\n",
              "      <td>-0.796738</td>\n",
              "      <td>-0.497731</td>\n",
              "      <td>1.963784</td>\n",
              "      <td>-0.284097</td>\n",
              "      <td>3.261419</td>\n",
              "      <td>-0.800969</td>\n",
              "      <td>-0.506469</td>\n",
              "      <td>-0.441910</td>\n",
              "      <td>0.079485</td>\n",
              "      <td>-0.463492</td>\n",
              "      <td>-0.471498</td>\n",
              "      <td>0.799019</td>\n",
              "      <td>-0.376445</td>\n",
              "      <td>-0.360156</td>\n",
              "      <td>-0.686314</td>\n",
              "      <td>-0.172683</td>\n",
              "      <td>1.956364</td>\n",
              "      <td>-0.891847</td>\n",
              "      <td>3.087634</td>\n",
              "      <td>-0.818431</td>\n",
              "      <td>0.335942</td>\n",
              "      <td>0.611970</td>\n",
              "      <td>0.758249</td>\n",
              "      <td>1.121906</td>\n",
              "      <td>0.802602</td>\n",
              "      <td>0.901728</td>\n",
              "      <td>3.559395</td>\n",
              "      <td>-1.312710</td>\n",
              "      <td>0.931835</td>\n",
              "      <td>2.984989</td>\n",
              "      <td>-0.468388</td>\n",
              "      <td>1.456828</td>\n",
              "      <td>0.267345</td>\n",
              "      <td>-0.318779</td>\n",
              "      <td>3.901358</td>\n",
              "      <td>-0.609226</td>\n",
              "      <td>-0.471357</td>\n",
              "      <td>1.560983</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.486394</td>\n",
              "      <td>0.011836</td>\n",
              "      <td>-0.607265</td>\n",
              "      <td>-0.756973</td>\n",
              "      <td>0.369215</td>\n",
              "      <td>-0.849208</td>\n",
              "      <td>2.028156</td>\n",
              "      <td>0.119567</td>\n",
              "      <td>-0.803319</td>\n",
              "      <td>-0.465819</td>\n",
              "      <td>0.024541</td>\n",
              "      <td>0.522723</td>\n",
              "      <td>-0.393713</td>\n",
              "      <td>0.456446</td>\n",
              "      <td>-1.160259</td>\n",
              "      <td>-0.269853</td>\n",
              "      <td>1.397479</td>\n",
              "      <td>0.577923</td>\n",
              "      <td>-0.016859</td>\n",
              "      <td>1.445956</td>\n",
              "      <td>-0.849506</td>\n",
              "      <td>2.242668</td>\n",
              "      <td>1.023136</td>\n",
              "      <td>1.804294</td>\n",
              "      <td>0.145638</td>\n",
              "      <td>-0.650150</td>\n",
              "      <td>1.232863</td>\n",
              "      <td>2.362357</td>\n",
              "      <td>0.294656</td>\n",
              "      <td>0.325273</td>\n",
              "      <td>-0.361375</td>\n",
              "      <td>0.469979</td>\n",
              "      <td>-0.755104</td>\n",
              "      <td>-0.431179</td>\n",
              "      <td>0.206783</td>\n",
              "      <td>2.577395</td>\n",
              "      <td>-0.640026</td>\n",
              "      <td>0.363521</td>\n",
              "      <td>0.624068</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.071460</td>\n",
              "      <td>-0.553484</td>\n",
              "      <td>0.029384</td>\n",
              "      <td>-0.145030</td>\n",
              "      <td>-0.509019</td>\n",
              "      <td>-0.443575</td>\n",
              "      <td>2.058645</td>\n",
              "      <td>-0.614785</td>\n",
              "      <td>-0.025296</td>\n",
              "      <td>-0.196204</td>\n",
              "      <td>1.508216</td>\n",
              "      <td>-0.386372</td>\n",
              "      <td>-0.322134</td>\n",
              "      <td>0.093845</td>\n",
              "      <td>-0.164296</td>\n",
              "      <td>-0.202908</td>\n",
              "      <td>-0.675448</td>\n",
              "      <td>1.987206</td>\n",
              "      <td>0.743067</td>\n",
              "      <td>-0.112193</td>\n",
              "      <td>1.952012</td>\n",
              "      <td>-0.279601</td>\n",
              "      <td>1.317924</td>\n",
              "      <td>1.228551</td>\n",
              "      <td>-0.335249</td>\n",
              "      <td>-0.084516</td>\n",
              "      <td>0.756525</td>\n",
              "      <td>1.279888</td>\n",
              "      <td>0.214162</td>\n",
              "      <td>-0.109619</td>\n",
              "      <td>1.804547</td>\n",
              "      <td>1.737462</td>\n",
              "      <td>-0.252732</td>\n",
              "      <td>1.941269</td>\n",
              "      <td>-0.171081</td>\n",
              "      <td>-0.347327</td>\n",
              "      <td>-0.324948</td>\n",
              "      <td>-0.587768</td>\n",
              "      <td>0.029561</td>\n",
              "      <td>-0.269150</td>\n",
              "      <td>...</td>\n",
              "      <td>1.275778</td>\n",
              "      <td>0.271461</td>\n",
              "      <td>2.004199</td>\n",
              "      <td>-0.408011</td>\n",
              "      <td>1.705920</td>\n",
              "      <td>-0.323414</td>\n",
              "      <td>-0.148019</td>\n",
              "      <td>0.805807</td>\n",
              "      <td>-0.330228</td>\n",
              "      <td>-0.294776</td>\n",
              "      <td>0.718698</td>\n",
              "      <td>0.251789</td>\n",
              "      <td>0.782802</td>\n",
              "      <td>-0.270511</td>\n",
              "      <td>-0.356032</td>\n",
              "      <td>-0.055300</td>\n",
              "      <td>0.247848</td>\n",
              "      <td>1.221817</td>\n",
              "      <td>-0.020865</td>\n",
              "      <td>1.573320</td>\n",
              "      <td>0.149294</td>\n",
              "      <td>-0.046459</td>\n",
              "      <td>0.353816</td>\n",
              "      <td>-0.122537</td>\n",
              "      <td>-0.027578</td>\n",
              "      <td>-0.480010</td>\n",
              "      <td>-0.243383</td>\n",
              "      <td>-0.362624</td>\n",
              "      <td>0.525934</td>\n",
              "      <td>-0.071147</td>\n",
              "      <td>-0.662112</td>\n",
              "      <td>-0.019017</td>\n",
              "      <td>-0.365846</td>\n",
              "      <td>-0.136268</td>\n",
              "      <td>0.428925</td>\n",
              "      <td>-0.099518</td>\n",
              "      <td>-0.303276</td>\n",
              "      <td>2.082908</td>\n",
              "      <td>0.965919</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.197739</td>\n",
              "      <td>-0.731074</td>\n",
              "      <td>-0.256347</td>\n",
              "      <td>-1.116051</td>\n",
              "      <td>-1.104760</td>\n",
              "      <td>0.012868</td>\n",
              "      <td>0.499145</td>\n",
              "      <td>-0.610310</td>\n",
              "      <td>-0.062483</td>\n",
              "      <td>-0.172752</td>\n",
              "      <td>-0.808338</td>\n",
              "      <td>-0.300046</td>\n",
              "      <td>-1.091365</td>\n",
              "      <td>-0.175848</td>\n",
              "      <td>-0.590491</td>\n",
              "      <td>-0.383289</td>\n",
              "      <td>-0.950313</td>\n",
              "      <td>1.537892</td>\n",
              "      <td>2.285882</td>\n",
              "      <td>-0.894574</td>\n",
              "      <td>1.005046</td>\n",
              "      <td>-0.548572</td>\n",
              "      <td>0.915498</td>\n",
              "      <td>2.209650</td>\n",
              "      <td>4.332871</td>\n",
              "      <td>1.777773</td>\n",
              "      <td>3.032871</td>\n",
              "      <td>2.438895</td>\n",
              "      <td>0.461631</td>\n",
              "      <td>1.467396</td>\n",
              "      <td>0.488518</td>\n",
              "      <td>0.534241</td>\n",
              "      <td>-0.472278</td>\n",
              "      <td>0.219244</td>\n",
              "      <td>0.072697</td>\n",
              "      <td>-0.203950</td>\n",
              "      <td>-0.583803</td>\n",
              "      <td>-0.649506</td>\n",
              "      <td>1.389083</td>\n",
              "      <td>-0.124601</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.562442</td>\n",
              "      <td>0.392000</td>\n",
              "      <td>0.149472</td>\n",
              "      <td>-0.060157</td>\n",
              "      <td>2.243619</td>\n",
              "      <td>-0.548696</td>\n",
              "      <td>1.862436</td>\n",
              "      <td>0.320670</td>\n",
              "      <td>-0.460364</td>\n",
              "      <td>-0.493218</td>\n",
              "      <td>-1.026154</td>\n",
              "      <td>0.948146</td>\n",
              "      <td>1.560519</td>\n",
              "      <td>2.732547</td>\n",
              "      <td>-1.321941</td>\n",
              "      <td>-0.210493</td>\n",
              "      <td>2.269151</td>\n",
              "      <td>1.818774</td>\n",
              "      <td>-0.473236</td>\n",
              "      <td>2.870932</td>\n",
              "      <td>-1.494021</td>\n",
              "      <td>-0.124955</td>\n",
              "      <td>0.283038</td>\n",
              "      <td>2.440798</td>\n",
              "      <td>1.405221</td>\n",
              "      <td>-1.211263</td>\n",
              "      <td>1.504962</td>\n",
              "      <td>0.168726</td>\n",
              "      <td>0.873936</td>\n",
              "      <td>2.443866</td>\n",
              "      <td>-0.567604</td>\n",
              "      <td>-0.187445</td>\n",
              "      <td>-0.365194</td>\n",
              "      <td>-0.753977</td>\n",
              "      <td>-0.016396</td>\n",
              "      <td>0.172039</td>\n",
              "      <td>-0.294501</td>\n",
              "      <td>0.093440</td>\n",
              "      <td>0.641316</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  513 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2         3  ...       509       510       511  label\n",
              "0  0.085607  0.021805  0.441109 -0.286769  ... -0.474211 -0.151722 -0.569436      1\n",
              "1 -0.133243 -0.964772 -0.479122 -0.388593  ... -0.952448  1.436085  1.016485      3\n",
              "2  0.083105 -0.696422 -0.796738 -0.497731  ... -0.640026  0.363521  0.624068      0\n",
              "3 -0.071460 -0.553484  0.029384 -0.145030  ... -0.303276  2.082908  0.965919      2\n",
              "4  0.197739 -0.731074 -0.256347 -1.116051  ... -0.294501  0.093440  0.641316      1\n",
              "\n",
              "[5 rows x 513 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hj5HSZOgTLcw",
        "outputId": "474d4225-1761-431b-a316-f8626ada4e23"
      },
      "source": [
        "ftrs_df.to_csv('./resnet50_FC_512_features_with_labels.csv',index=False)\r\n",
        "\r\n",
        "print('feature set saved successfully !')"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "feature set saved successfully !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNh3ZlMILLMZ"
      },
      "source": [
        "# save model\r\n",
        "MODEL_PATH = './resnet50_TL_model_94%acc.pth'\r\n",
        "torch.save(model.state_dict(),MODEL_PATH)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3AQ3G6RQDDK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}